# What does it mean to analyse?

When a new document is added, the full text fields (`text` fields) undergo an analysis process.

This involves tokenizing text in terms, normalizing the text, lower-casing text, etc.

This is done to make the text easier to search, and can be controlled by choosing which type of analyser to use.

# a closer look at analysers

an analyser consists of:

1. **character filters, e.g.**
   1.  `pattern_replace`, uses regex to match characters and replace them with specified replacement.
   2. `mapping`, replaces values based on map of keys and values
   3. `html_strip`, strip out html elements like `<strong>` and decodes HTML entities like `&amp`.
2. **tokenizer, e.g.:**
   1. word-oriented tokenizers - typically used for tokenizing full text into individual **words**.
      1. `standard`: divides text into terms on word boundaries and removes most symbols. **usually the best choice**.
      2. `letter`: divides text into terms when encountering character that is not an alphabet letter.
      3. `lowercase`: just like `letter`, but also lowercases all terms.
      4. `whitespace`: `string.split(' ')`
      5. `uax_url_email`: like `standard`, but treats URLs and e-mail addresses as single tokens.
   2. partial word tokenizers - breaks up text or words into small fragments. Used for partial word matching.
      1. `ngram`: breaks text into words when encountering certain characters and then emits N-grams (syllabic / phoneme based 2 - 10 characters in length) of the specified length
      2. `edge_ngram`- this is used sometimes for auto-completion, but suggesters are better. see more by looking at ES website.
   3. structured text tokenizers - used for structured text such as e-mail addresses, zip codes, identifiers, etc.
      1. `keyword` - no-op tokenizer which outputs the exact same text as a single term (doesn't tokenize anything at all)
      2. `pattern` - uses regex to split text into terms when matching a word separator. Alternatively captures matched text as terms.
      3. `path_hierarchy` - splits hierarchical values (e.g. file system paths) and emits a term for each component in the tree.
         1. e.g. `/path/to/some/directory` becomes [`/path`, `/path/to`, `/path/to/some`, etc..]
3. **token filters**
   1. `standard`
      1. doesn't do anything. 
   2. `lowercase` - lowercases all terms
   3. `uppercase` - uppercases all terms
   4. `nGram` - emits N-grams of specified length based on the provided terms. (there is an ngram token filter so that we can use custom tokenizers and then make ngram representations)
   5. `edgeNGram` - same as above
   6. `stop` - removes stop words (redundant english words) like `for`, `in`, `the`
   7. `word_delimiter` - splits words into subwords and performs transformations on subword groups. e.g. `PowerShell -> power, shell`
   8. `stemmer` - stems words for the specified language
      1. e.g. `drinking -> drink`
   9. `snowball` - stem words based on snowball algorithm (A string processing algorithm)
   10. `synonym` - handles synonyms 

![ image-20181229002315741](/Users/willyspinner/Desktop/elastic_tuts/complete-guide-to-elasticsearch/3. Analysis & Analyzers/assets/image-20181229002315741.png)

token filters are filters applied to token arrays. 

